{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ciAvre3l1HA"
      },
      "source": [
        "import torch     \n",
        "from torch import Tensor        \n",
        "import torch.autograd as autograd           \n",
        "import torch.nn as nn                   \n",
        "import torch.nn.functional as F           \n",
        "import torch.optim as optim      \n",
        "from torch.distributions import Categorical         \n",
        "import gym\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt73VW6tl75j"
      },
      "source": [
        "class PPO(nn.Module):\n",
        "  def __init__(self, inp_size, out_size):\n",
        "    super(PPO, self).__init__()\n",
        "    self.ac_fc1 = nn.Linear(inp_size, 128)\n",
        "    self.ac_fc2 = nn.Linear(128, 128)\n",
        "    self.ac_fc3 = nn.Linear(128, out_size)\n",
        "\n",
        "    self.cr_fc1 = nn.Linear(inp_size, 128)\n",
        "    self.cr_fc2 = nn.Linear(128, 128)\n",
        "    self.cr_fc3 = nn.Linear(128, 1)\n",
        "  def forward(self, obs):\n",
        "    #actor\n",
        "    x= F.relu(self.ac_fc1(obs))\n",
        "    x= F.relu(self.ac_fc2(x))\n",
        "    x = self.ac_fc3(x)\n",
        "\n",
        "    #critic\n",
        "    y= F.relu(self.cr_fc1(obs))\n",
        "    y= F.relu(self.cr_fc2(y))\n",
        "    y = self.cr_fc3(y)\n",
        "\n",
        "\n",
        "    return y, F.softmax(x)   #y(critic) is used as estimate for value function. and softmax(x) (actor) is used to select action"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ove-MQwpnAGX"
      },
      "source": [
        "def ratios(act_prob, old_act_prob, EPSILON):\n",
        "  #calculate ratio and clipped ratio(takes in batches or torch...)\n",
        "  ratio = torch.exp(act_prob - old_act_prob)\n",
        "  clipped_ratio = torch.clamp(ratio, 1-EPSILON, 1+EPSILON) #does what it sounds like\n",
        "  return ratio, clipped_ratio\n",
        "\n",
        "def train():   #if you can do without .item() please do, otherwise you are returning a scalar value and we still need a tensor from the network to perform backprop\n",
        "  #hyperparameters\n",
        "  LR=0.01\n",
        "  GAMMA =0.99\n",
        "  EPSILON = 0.2\n",
        "  c1 = 1\n",
        "  c2 =0.01\n",
        "  EPOCHS= 3 #num of epochs to train net before next episode\n",
        "  MseLoss = nn.MSELoss()\n",
        "\n",
        "  #make environment\n",
        "  env = gym.make('Breakout-ram-v0')  \n",
        "  obs=env.reset()\n",
        "  done = False\n",
        "                                                  \n",
        "  net = PPO(env.observation_space.shape[0],env.action_space.n)\n",
        "  old_net = PPO(env.observation_space.shape[0],env.action_space.n)\n",
        "  old_net.load_state_dict(net.state_dict())\n",
        "  old_net.eval()\n",
        "\n",
        "  optimizer=optim.Adam(net.parameters(), lr=LR)\n",
        "  \n",
        "  old_act_tensors = torch.empty(0)\n",
        "  old_act_prob_tensors = torch.empty(0)\n",
        "  obs_tensors = torch.empty(0)\n",
        "  obs_list = []\n",
        "  rewards_list=[]\n",
        "\n",
        "  count = 0                    #count number of episodes\n",
        "  num_episodes = 100          #max number of episodes\n",
        "  cum_reward=0                      #for visualization purposes\n",
        "  \n",
        "  while (True):\n",
        "    env.render()\n",
        "    _, old_action_softmax = old_net(torch.from_numpy(obs).float())   #we are choosing our action based on the old policy\n",
        "    old_act_distribution = Categorical(old_action_softmax)\n",
        "    old_action = old_act_distribution.sample()\n",
        "    old_act_prob = old_act_distribution.log_prob(old_action)\n",
        "\n",
        "    old_act_prob_tensors = torch.cat((old_act_prob_tensors, torch.unsqueeze(old_act_prob,0)),0)\n",
        "    old_act_tensors = torch.cat((old_act_tensors,torch.unsqueeze(old_action,0)),0)\n",
        "    obs_list.append(obs)\n",
        "\n",
        "    obs, reward, done, info = env.step(old_action.item())\n",
        "\n",
        "    reward = reward if (not done) else 0\n",
        "    rewards_list.append(reward)\n",
        "\n",
        "    cum_reward+=reward\n",
        "\n",
        "    if done:\n",
        "      print(cum_reward)\n",
        "      cum_reward=0\n",
        "      obs = env.reset()\n",
        "\n",
        "      #since we are going to be getting outputs as batch, its best we convert to tensors\n",
        "      reward_tensors = torch.empty(0)\n",
        "      discount_r = 0\n",
        "      temp_r = []\n",
        "      for rewd in reversed(rewards_list):\n",
        "        discount_r = rewd + GAMMA*discount_r\n",
        "        temp_r.append(discount_r)\n",
        "      temp_r = list(reversed(temp_r))\n",
        "      reward_tensors = torch.FloatTensor(temp_r)   \n",
        "\n",
        "      obs_tensors = torch.tensor(obs_list).float()\n",
        "\n",
        "      for _ in range(EPOCHS):\n",
        "        value, action_softmax = net(obs_tensors)  \n",
        "        value = torch.squeeze(value,1)      \n",
        "        act_distribution = Categorical(action_softmax)\n",
        "        act_prob = act_distribution.log_prob(old_act_tensors)\n",
        "\n",
        "        #calculate ratio and clipped ratio\n",
        "\n",
        "        ratio, clipped_ratio = ratios(act_prob, old_act_prob_tensors.detach(), EPSILON) #detach because wrt to current theta...also pytorch doesnt let it work\n",
        "\n",
        "        entropy = act_distribution.entropy().mean()\n",
        "\n",
        "        advantage = reward_tensors - value\n",
        "\n",
        "        #calculate surrogate loss        \n",
        "\n",
        "        loss = -(torch.min(ratio*advantage,clipped_ratio*advantage) - c1*MseLoss(reward_tensors, value) + c2*entropy)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      #reset contiainers\n",
        "      old_act_prob_tensors = torch.empty(0)\n",
        "      old_act_tensors = torch.empty(0)\n",
        "      obs_tensors = torch.empty(0)\n",
        "      rewards_list=[] \n",
        "      obs_list =[]\n",
        "\n",
        "      #update old network\n",
        "      old_net.load_state_dict(net.state_dict())\n",
        "\n",
        "      count+=1\n",
        "      if (count==num_episodes):\n",
        "        print('done!')\n",
        "        env.close()\n",
        "        break"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gYviCihl_BG",
        "outputId": "f036bd72-f806-437a-c03f-02fe719b25be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "tags": []
      },
      "source": [
        "train()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.0\n0.0\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwPcYGS2roW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}